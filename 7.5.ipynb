{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytorch_lightning --quiet\n",
    "!pip install wandb --quiet\n",
    "!pip install pandas --quiet\n",
    "!pip install numpy --quiet\n",
    "!pip install scikit-learn --quiet\n",
    "!pip install matplotlib --quiet\n",
    "!pip install seaborn --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import wandb\n",
    "import pprint\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import random\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x18f39d3f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reproducibility\n",
    "generator = torch.Generator()\n",
    "generator.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "file_1_dataset  = \"/content/mydrive/MyDrive/ColabNotebooks/EAI_Napoli/Dataset/Copia di Copy of fused_transposed_GG1.csv\"\n",
    "dataset_path_list = [\n",
    "    'Copia di S1E1.csv', # Task 4 --> 0\n",
    "    'Copia di S1E2.csv', # Baseline 1 --> 1\n",
    "    'Copia di S1E3.csv', # Baseline 2 --> 2\n",
    "    'Copia di S1E4.csv', # Task 1 --> 3\n",
    "    'Copia di S2E1.csv', # Baseline 1 --> 4\n",
    "    'Copia di S2E2.csv', # Baseline 2 --> 5\n",
    "    'Copia di S2E3.csv', # Task 2 --> 6\n",
    "    'Copia di S2E4.csv', # Task 1 --> 7\n",
    "    'Copia di S3E1.csv', # Baseline 1 --> 8\n",
    "    'Copia di S3E2.csv', # Task 1 --> 9\n",
    "    'Copia di S3E3.csv', # Task 3 --> 10\n",
    "    'Copia di S3E4.csv', # Task 2 --> 11\n",
    "    'Copia di S4E1.csv', # Task 1 --> 12\n",
    "    'Copia di S4E2.csv', # Task 1 --> 13\n",
    "    'Copia di S4E3.csv', # Task 3  --> 14\n",
    "    'Copia di S4E4.csv', # Task 3 --> 15\n",
    "    'Copia di S5E1.csv', # Task 1 --> 16\n",
    "    'Copia di S5E2.csv', # Baseline 1 --> 17\n",
    "    'Copia di S5E3.csv', # Task 1 --> 18\n",
    "    'Copia di S5E4.csv', # Task 4 --> 19\n",
    "    'Copia di S6E1.csv', # Task 1 --> 20\n",
    "    'Copia di S6E2.csv', # Task 3 --> 21\n",
    "    'Copia di S6E3.csv', # Baseline 2 --> 22\n",
    "    'Copia di S6E4.csv', # Task 3 --> 23\n",
    "    'Copia di S7E1.csv', # Task 2 --> 24\n",
    "    'Copia di S7E2.csv', # Task 1 --> 25\n",
    "    'Copia di S7E3.csv', # Task 4 --> 26\n",
    "    'Copia di S7E4.csv', # Task 1 --> 27\n",
    "]\n",
    "\n",
    "batch_size = 32\n",
    "window_size = 1 # size of the window to consider when selecting a sample, then a sample will be composed of window_size rows\n",
    "enable_wandb = False\n",
    "\n",
    "dataset_base_path = \"Dataset\"\n",
    "project_base_path = \"/\"\n",
    "\n",
    "\n",
    "# Definition of an utity object that divides the dataset files according to the task they belong to\n",
    "dataset_task_mapping = {}\n",
    "dataset_task_mapping['task_1'] = []\n",
    "\n",
    "# Task 1\n",
    "task_1_file_index = [3, 7, 9, 12, 13, 16, 18, 20, 25, 27]\n",
    "for index in task_1_file_index:\n",
    "    dict_file = {}\n",
    "    dict_file['file_path'] = os.path.join(dataset_base_path, dataset_path_list[index])\n",
    "    #file_name\n",
    "    dict_file['file_name'] = dataset_path_list[index].split(\"/\")[-1]\n",
    "    #remove the extension\n",
    "    dict_file['file_name'] = dict_file['file_name'].split(\".\")[0]\n",
    "    dataset_task_mapping['task_1'].append(dict_file)\n",
    "\n",
    "# Task 2\n",
    "dataset_task_mapping['task_2'] = []\n",
    "task_2_file_index = [6, 11, 24]\n",
    "for index in task_2_file_index:\n",
    "    dict_file = {}\n",
    "    dict_file['file_path'] = os.path.join(dataset_base_path, dataset_path_list[index])\n",
    "    dict_file['file_name'] = dataset_path_list[index].split(\"/\")[-1]\n",
    "    dict_file['file_name'] = dict_file['file_name'].split(\".\")[0]\n",
    "    dataset_task_mapping['task_2'].append(dict_file)\n",
    "\n",
    "# Task 3\n",
    "dataset_task_mapping['task_3'] = []\n",
    "task_3_file_index = [10, 14, 21, 23, 26]\n",
    "for index in task_3_file_index:\n",
    "    dict_file = {}\n",
    "    dict_file['file_path'] = os.path.join(dataset_base_path, dataset_path_list[index])\n",
    "    dict_file['file_name'] = dataset_path_list[index].split(\"/\")[-1]\n",
    "    dict_file['file_name'] = dict_file['file_name'].split(\".\")[0]\n",
    "    dataset_task_mapping['task_3'].append(dict_file)\n",
    "\n",
    "# Task 4\n",
    "dataset_task_mapping['task_4'] = []\n",
    "task_4_file_index = [0, 19]\n",
    "for index in task_4_file_index:\n",
    "    dict_file = {}\n",
    "    dict_file['file_path'] = os.path.join(dataset_base_path, dataset_path_list[index])\n",
    "    dict_file['file_name'] = dataset_path_list[index].split(\"/\")[-1]\n",
    "    dict_file['file_name'] = dict_file['file_name'].split(\".\")[0]\n",
    "    dataset_task_mapping['task_4'].append(dict_file)\n",
    "\n",
    "# Baseline 1\n",
    "dataset_task_mapping['baseline_1'] = []\n",
    "baseline_1_file_index = [1, 4, 8, 17]\n",
    "for index in baseline_1_file_index:\n",
    "    dict_file = {}\n",
    "    dict_file['file_path'] = os.path.join(dataset_base_path, dataset_path_list[index])\n",
    "    dict_file['file_name'] = dataset_path_list[index].split(\"/\")[-1]\n",
    "    dict_file['file_name'] = dict_file['file_name'].split(\".\")[0]\n",
    "    dataset_task_mapping['baseline_1'].append(dict_file)\n",
    "\n",
    "# Baseline 2\n",
    "dataset_task_mapping['baseline_2'] = []\n",
    "baseline_2_file_index = [2, 5, 22]\n",
    "for index in baseline_2_file_index:\n",
    "    dict_file = {}\n",
    "    dict_file['file_path'] = os.path.join(dataset_base_path, dataset_path_list[index])\n",
    "    dict_file['file_name'] = dataset_path_list[index].split(\"/\")[-1]\n",
    "    dict_file['file_name'] = dict_file['file_name'].split(\".\")[0]\n",
    "    dataset_task_mapping['baseline_2'].append(dict_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetApproach1(Dataset):\n",
    "  def __init__(self, task_name, window_size=1):\n",
    "    assert task_name in ['task_1','task_2','task_3','task_4'] , \"The task_name is not valid (must be one of ['task_1','task_2','task_3','task_4'])\"\n",
    "\n",
    "    self.task_name = task_name\n",
    "    self.dataset = []\n",
    "\n",
    "    # Create a unique dataframe that is composed by the concatenation of all the files that belong to the task + the baselines files\n",
    "    for file in dataset_task_mapping[task_name]:\n",
    "      df = pd.read_csv(file['file_path'])\n",
    "      self.dataset.append(df)\n",
    "\n",
    "    # Baseline 1\n",
    "    for file in dataset_task_mapping['baseline_1']:\n",
    "      df = pd.read_csv(file['file_path'])\n",
    "      self.dataset.append(df)\n",
    "\n",
    "    # Baseline 2\n",
    "    for file in dataset_task_mapping['baseline_2']:\n",
    "      df = pd.read_csv(file['file_path'])\n",
    "      self.dataset.append(df)\n",
    "\n",
    "    # Concatenate the dataframes\n",
    "    print(f\"Concatenating the dataframes ({len(self.dataset)})\")\n",
    "    self.dataset = pd.concat(self.dataset)\n",
    "    # Create a dataframe\n",
    "    print(f\"Dataset shape: {self.dataset.shape}\")\n",
    "\n",
    "\n",
    "    # Windowing\n",
    "    self.window_size = window_size\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.dataset) - self.window_size\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    # return as a tensor\n",
    "    print(f\"Index: {idx}\")\n",
    "    return torch.tensor(self.dataset.iloc[idx].values)\n",
    "\n",
    "  def get_dataframe(self):\n",
    "    return self.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "selected_task = 'task_1'\n",
    "dataset = DatasetApproach1(selected_task, window_size=window_size)\n",
    "print(f\"Dataset length: {len(dataset)}, Number of files used ({len(dataset_task_mapping[selected_task])} + {len(dataset_task_mapping['baseline_1'])} + {len(dataset_task_mapping['baseline_2'])})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "selected_task = 'task_2'\n",
    "dataset = DatasetApproach1(selected_task, window_size=window_size)\n",
    "print(f\"Dataset length: {len(dataset)}, Number of files used ({len(dataset_task_mapping[selected_task])} + {len(dataset_task_mapping['baseline_1'])} + {len(dataset_task_mapping['baseline_2'])})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "selected_task = 'task_3'\n",
    "dataset = DatasetApproach1(selected_task, window_size=window_size)\n",
    "print(f\"Dataset length: {len(dataset)}, Number of files used ({len(dataset_task_mapping[selected_task])} + {len(dataset_task_mapping['baseline_1'])} + {len(dataset_task_mapping['baseline_2'])})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "selected_task = 'task_4'\n",
    "dataset = DatasetApproach1(selected_task, window_size=window_size)\n",
    "print(f\"Dataset length: {len(dataset)}, Number of files used ({len(dataset_task_mapping[selected_task])} + {len(dataset_task_mapping['baseline_1'])} + {len(dataset_task_mapping['baseline_2'])})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unique values of column \"labels\"\n",
    "print(f\"Unique values of the labels: {dataset.get_dataframe()['labels'].unique()}\")\n",
    "\n",
    "# Shuffle the rows of the dataset using sklearn (making sure the shuffle is reproducible)\n",
    "from sklearn.utils import shuffle\n",
    "data = shuffle(dataset.get_dataframe(), random_state=0)\n",
    "# Remove the index column\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "# Splitting into train and test sets (80% training data, 20% testing data)\n",
    "train_df, test_df = train_test_split(data, test_size=0.15, random_state=42)\n",
    "\n",
    "# Splitting the train_df further into train and validation sets (70% training data, 30% validation data)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.15, random_state=42)\n",
    "\n",
    "print(f\"Data: {len(data)} ,Train size: {len(train_df)}, Val size: {len(val_df)}, Test size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the Dataframe classe Approach 1\n",
    "class DataFrameApproach1(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe.iloc[:, :-1].values\n",
    "        self.targets = dataframe['labels'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.data[idx])\n",
    "        y = self.targets[idx]\n",
    "        return x, y\n",
    "\n",
    "def collate_fn(batch):\n",
    "    data = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "\n",
    "    # Apply min-max normalization to each column\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_data = scaler.fit_transform(data)\n",
    "\n",
    "    return torch.tensor(normalized_data), targets\n",
    "\n",
    "# Creating datasets and data loaders for each split\n",
    "train_dataset = DataFrameApproach1(train_df)\n",
    "val_dataset = DataFrameApproach1(val_df)\n",
    "test_dataset = DataFrameApproach1(test_df)\n",
    "\n",
    "# NOTE -> Dataloader are created in the HPO sweeps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoEncoder Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, window_size = 1, enable_sparsity_loss=False):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(128),\n",
    "        )\n",
    "\n",
    "        # Apply He initialization to the linear layers\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, input_dim = x.size()  # Obtain the shape of the input [bs, input_dim]\n",
    "        input = x\n",
    "        x = self.encoder(input)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim, window_size, enable_sparsity_loss=False):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Linear(256, window_size * input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Apply He initialization to the linear layers\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = x.to(torch.float32)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AE Lightning Module âš¡ï¸âš¡ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(LightningModule):\n",
    "    def __init__(self, input_dim, batch_size, sparsity_factor=0.1, sparsity_loss_coef = 1e-3, weight_decay=0.001, window_size=window_size, enable_sparsity_loss=False, enable_weight_decay_loss=False ,enable_non_negativity_constraint=False,enable_wandb = False):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        if( enable_sparsity_loss == True and enable_non_negativity_constraint== True):\n",
    "          print(\"The combination of constraints enable_sparsity_loss and enable_non_negativity_constraint both true leads to error in to the model matrix multiplication. This will be solved by setting enable_non_negativity_constraint to False.\")\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        self.encoder = Encoder(input_dim=input_dim, window_size=window_size, enable_sparsity_loss = enable_sparsity_loss)\n",
    "        self.decoder = Decoder(input_dim=input_dim, window_size=window_size, enable_sparsity_loss = enable_sparsity_loss)\n",
    "        self.train_loss_memory = []\n",
    "        self.train_rec_loss_memory = []\n",
    "\n",
    "        self.val_loss_memory = []\n",
    "        self.val_rec_loss_memory = []\n",
    "\n",
    "        self.test_loss_memory = []\n",
    "        self.test_rec_loss_memory = []\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "\n",
    "\n",
    "        # --- Loss Settings\n",
    "        self.enable_sparsity_loss = enable_sparsity_loss\n",
    "        if enable_sparsity_loss:\n",
    "          self.sparsity_loss_coef = sparsity_loss_coef\n",
    "          self.sparsity_factor = sparsity_factor\n",
    "          print(f\"Enabled Sparsity term in the loss with sparsity loss coeff => {self.sparsity_loss_coef} and sparsity factor=>{self.sparsity_factor}\")\n",
    "\n",
    "          # self.sparsity_loss = nn.KLDivLoss(reduction='batchmean')\n",
    "          # Memory logs for sparsity\n",
    "          self.train_sparsity_loss_memory = []\n",
    "          self.val_sparsity_loss_memory = []\n",
    "          self.test_sparsity_loss_memory = []\n",
    "\n",
    "          self.enable_non_negativity_constraint = False\n",
    "        else:\n",
    "          self.enable_non_negativity_constraint = enable_non_negativity_constraint\n",
    "          if enable_non_negativity_constraint:\n",
    "            print(\"Enabled non negativity constraint\")\n",
    "\n",
    "\n",
    "        self.enable_weight_decay_loss = enable_weight_decay_loss\n",
    "        if enable_weight_decay_loss:\n",
    "          print(\"Enabled weight decay\")\n",
    "          self.weight_decay = weight_decay\n",
    "\n",
    "        self.wandb_log = enable_wandb\n",
    "\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        if torch.cuda.is_available():\n",
    "            if torch.cuda.device_count() > 1:\n",
    "                device = torch.device('cuda:0')\n",
    "                print('Using device:', device)\n",
    "            else:\n",
    "                device = torch.device('cuda')\n",
    "                print('Using device:', device)\n",
    "        else:\n",
    "            device = torch.device('cpu')\n",
    "            print('Using device:', device)\n",
    "\n",
    "\n",
    "        print('Using device:', device)\n",
    "\n",
    "        self.to(device)\n",
    "        print(f\"Initialized Model on {self.device}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "\n",
    "    def kl_div(self, p, p_hat):\n",
    "      funcs = nn.Sigmoid()\n",
    "      p_hat = torch.mean(funcs(p_hat), 1)\n",
    "      p_tensor = torch.Tensor([p] * p_hat.shape[0]).to(self.device)\n",
    "\n",
    "\n",
    "      return torch.sum(p_tensor * torch.log(p_tensor) - p_tensor * torch.log(p_hat) + (1 - p_tensor) * torch.log(1 - p_tensor) - (1 - p_tensor) * torch.log(1 - p_hat))\n",
    "\n",
    "    def sparse_loss(self, values):\n",
    "      loss = 0\n",
    "      values = values.view(self.batch_size, -1)\n",
    "\n",
    "      # Encoder sparsity\n",
    "      lyrs_encoder = list(self.encoder.encoder.children())\n",
    "      for i, lyr in enumerate(lyrs_encoder):\n",
    "          if isinstance(lyr, nn.Linear):\n",
    "            values = lyr(values)\n",
    "            # loss += self.sparsity_loss(torch.tensor([self.sparsity_factor]).to(self.device), values.to(self.device))\n",
    "            loss += self.kl_div(self.sparsity_factor, values.to(self.device))\n",
    "\n",
    "      # Decoder sparsity\n",
    "      lyrs_decoder = list(self.decoder.decoder.children())\n",
    "      for i, lyr in enumerate(lyrs_decoder):\n",
    "          if isinstance(lyr, nn.Linear):\n",
    "              values = lyr(values)\n",
    "              # loss += self.sparsity_loss(torch.tensor([self.sparsity_factor]).to(self.device), values.to(self.device))\n",
    "              loss += self.kl_div(self.sparsity_factor, values.to(self.device))\n",
    "\n",
    "      return loss\n",
    "\n",
    "    def calculate_weight_decay_loss(self):\n",
    "        weight_decay_loss = 0.0\n",
    "        for param in self.parameters():\n",
    "            weight_decay_loss += 0.5 * self.weight_decay * torch.norm(param, p=2) ** 2\n",
    "        return weight_decay_loss\n",
    "\n",
    "    def enforce_non_negativity(self):\n",
    "      for param in self.parameters():\n",
    "        param.data.clamp_(min=0, max=None)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch[0].to(torch.float32) #[bs, input_dim]\n",
    "        _, reconstructions = self(x)\n",
    "\n",
    "        x = x.view(-1) # [bs * input_dim]\n",
    "        reconstructions = reconstructions.view(-1)\n",
    "\n",
    "        loss_mse = nn.MSELoss()(reconstructions, x)\n",
    "        loss = loss_mse\n",
    "\n",
    "        if self.enable_sparsity_loss:\n",
    "          # sparsity_loss = self.sparsity_loss(torch.log(reconstructions).to(self.device), torch.tensor([self.sparsity_factor]).to(self.device))\n",
    "          sparsity_loss = self.sparse_loss(x) * self.sparsity_loss_coef\n",
    "          loss += sparsity_loss\n",
    "          self.train_sparsity_loss_memory.append(sparsity_loss)\n",
    "\n",
    "        if self.enable_weight_decay_loss:\n",
    "          weight_decay_loss = self.calculate_weight_decay_loss()\n",
    "          loss += weight_decay_loss\n",
    "\n",
    "        self.train_loss_memory.append(loss)\n",
    "        self.train_rec_loss_memory.append(loss_mse)\n",
    "\n",
    "        if self.wandb_log:\n",
    "          wandb.log({\"train_total_loss\": loss})\n",
    "          wandb.log({\"train_reconstruction_loss\": loss_mse})\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "      x = batch[0].to(torch.float32)\n",
    "      _, reconstructions = self(x)\n",
    "\n",
    "      x = x.view(-1) #[]\n",
    "      reconstructions = reconstructions.view(-1)\n",
    "\n",
    "      loss_mse = nn.MSELoss()(reconstructions, x)\n",
    "      loss = loss_mse\n",
    "\n",
    "      if self.enable_sparsity_loss:\n",
    "        # sparsity_loss = self.sparsity_loss(torch.log(reconstructions).to(self.device), torch.tensor([self.sparsity_factor]).to(self.device))\n",
    "        sparsity_loss = self.sparse_loss(x) * self.sparsity_loss_coef\n",
    "        loss += sparsity_loss\n",
    "        self.val_sparsity_loss_memory.append(sparsity_loss)\n",
    "\n",
    "      if self.enable_weight_decay_loss:\n",
    "        weight_decay_loss = self.calculate_weight_decay_loss()\n",
    "        loss += weight_decay_loss\n",
    "\n",
    "      if self.enable_non_negativity_constraint:\n",
    "        self.enforce_non_negativity()\n",
    "\n",
    "      self.val_loss_memory.append(loss)\n",
    "      self.val_rec_loss_memory.append(loss_mse)\n",
    "\n",
    "      if self.wandb_log:\n",
    "        wandb.log({\"val_total_loss\": loss})\n",
    "        wandb.log({\"val_reconstruction_loss\": loss_mse})\n",
    "\n",
    "\n",
    "      return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "      x = batch[0].to(torch.float32)\n",
    "      _, reconstructions = self(x)\n",
    "\n",
    "      x = x.view(-1) #[]\n",
    "      reconstructions = reconstructions.view(-1)\n",
    "\n",
    "      loss_mse = nn.MSELoss()(reconstructions, x)\n",
    "      loss = loss_mse\n",
    "\n",
    "      if self.enable_sparsity_loss:\n",
    "        # sparsity_loss = self.sparsity_loss(torch.log(reconstructions).to(self.device), torch.tensor([self.sparsity_factor]).to(self.device))\n",
    "        sparsity_loss = self.sparse_loss(x) * self.sparsity_loss_coef\n",
    "        loss += sparsity_loss\n",
    "        self.test_sparsity_loss_memory.append(sparsity_loss)\n",
    "\n",
    "      if self.enable_weight_decay_loss:\n",
    "        weight_decay_loss = self.calculate_weight_decay_loss()\n",
    "        loss += weight_decay_loss\n",
    "\n",
    "      if self.enable_non_negativity_constraint:\n",
    "        self.enforce_non_negativity()\n",
    "\n",
    "      self.test_loss_memory.append(loss)\n",
    "      self.test_rec_loss_memory.append(loss_mse)\n",
    "\n",
    "\n",
    "      return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=10)  # Adjust T_max as needed\n",
    "\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': {'scheduler': scheduler, 'interval': 'epoch'}}\n",
    "\n",
    "    def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_closure):\n",
    "        # step\n",
    "        optimizer.step(closure=optimizer_closure)\n",
    "\n",
    "        if self.enable_non_negativity_constraint:\n",
    "          self.enforce_non_negativity()\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.wandb_log:\n",
    "            wandb.log({'epoch': self.current_epoch})\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        # Access the training loss from the outputs\n",
    "        train_loss = torch.stack([x for x in self.train_loss_memory]).mean()\n",
    "        train_rec_loss = torch.stack([x for x in self.train_rec_loss_memory]).mean()\n",
    "\n",
    "        # Print the training loss\n",
    "        print_log = f'Training Loss - Epoch {self.current_epoch}: Total Loss => {train_loss.item()} MSE => {train_rec_loss}'\n",
    "\n",
    "        self.train_loss_memory.clear()\n",
    "        self.train_rec_loss_memory.clear()\n",
    "\n",
    "        if self.enable_sparsity_loss:\n",
    "          train_sparsity_loss = torch.stack([x for x in self.train_sparsity_loss_memory]).mean()\n",
    "          print_log += f' SPARSE => {train_sparsity_loss}'\n",
    "          self.train_sparsity_loss_memory.clear()\n",
    "\n",
    "        if self.wandb_log:\n",
    "          wandb.log({\"train_total_loss\": train_loss})\n",
    "          wandb.log({\"train_reconstruction_loss\": train_rec_loss})\n",
    "          if self.enable_sparsity_loss:\n",
    "            wandb.log({\"train_sparse_loss\": train_sparsity_loss})\n",
    "\n",
    "        print(print_log)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        # Access the training loss from the outputs\n",
    "        val_loss = torch.stack([x for x in self.val_loss_memory]).mean()\n",
    "        val_rec_loss = torch.stack([x for x in self.val_rec_loss_memory]).mean()\n",
    "\n",
    "        # Print the training loss\n",
    "        print_log = f'Validation Loss - Epoch {self.current_epoch}: Total Loss => {val_loss.item()} MSE => {val_rec_loss}'\n",
    "\n",
    "        # For early stop and Model checkpoint callbacks\n",
    "        self.log('val_reconstruction_loss', val_rec_loss.item())\n",
    "\n",
    "\n",
    "        self.val_loss_memory.clear()\n",
    "        self.val_rec_loss_memory.clear()\n",
    "\n",
    "        if self.enable_sparsity_loss:\n",
    "          val_sparsity_loss = torch.stack([x for x in self.val_sparsity_loss_memory]).mean()\n",
    "          print_log += f' SPARSE => {val_sparsity_loss}'\n",
    "          self.val_sparsity_loss_memory.clear()\n",
    "\n",
    "        if self.wandb_log:\n",
    "          wandb.log({\"val_total_loss\": val_loss})\n",
    "          wandb.log({\"val_reconstruction_loss\": val_rec_loss})\n",
    "          if self.enable_sparsity_loss:\n",
    "            wandb.log({\"val_sparse_loss\": val_sparsity_loss})\n",
    "\n",
    "\n",
    "        print(print_log)\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        # Access the training loss from the outputs\n",
    "        test_loss = torch.stack([x for x in self.test_loss_memory]).mean()\n",
    "        test_rec_loss = torch.stack([x for x in self.test_rec_loss_memory]).mean()\n",
    "\n",
    "        # Print the training loss\n",
    "        print_log = f'Test Loss - Epoch {self.current_epoch}: Total Loss => {test_loss.item()} MSE => {test_rec_loss}'\n",
    "\n",
    "        self.test_loss_memory.clear()\n",
    "        self.test_rec_loss_memory.clear()\n",
    "\n",
    "        if self.enable_sparsity_loss:\n",
    "          test_sparsity_loss = torch.stack([x for x in self.test_sparsity_loss_memory]).mean()\n",
    "          print_log += f' SPARSE => {test_sparsity_loss}'\n",
    "          self.test_sparsity_loss_memory.clear()\n",
    "\n",
    "        if self.wandb_log:\n",
    "          wandb.log({\"test_total_loss\": test_loss})\n",
    "          wandb.log({\"test_reconstruction_loss\": test_rec_loss})\n",
    "          if self.enable_sparsity_loss:\n",
    "            wandb.log({\"test_sparse_loss\": test_sparsity_loss})\n",
    "\n",
    "        self.test_rec_loss = test_rec_loss\n",
    "\n",
    "        print(print_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING: Hyper Parameter Optimization with Weights and Biases Sweeps ðŸ”ŽðŸ”Ž"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WANDB Sweep for HPO\n",
    "sweep_config = {\n",
    "    'method': 'bayes'\n",
    "}\n",
    "metric = {\n",
    "  'name': 'val_reconstruction_loss',\n",
    "  'goal': 'minimize'\n",
    "}\n",
    "sweep_config['metric'] = metric\n",
    "parameters_dict = {\n",
    "    'batch_size': {\n",
    "          'values': [64]\n",
    "        },\n",
    "    'epochs': {\n",
    "          'values': [1000]\n",
    "        },\n",
    "    'sparsity_factor': {\n",
    "        'values': [0.1, 0.05, 0.005]\n",
    "      },\n",
    "    'wdecay_loss':{\n",
    "        'values': [True,False]\n",
    "      },\n",
    "    'sparsity_loss':{\n",
    "        'values': [True,False]\n",
    "      },\n",
    "    'non_negative_constraint':{\n",
    "        'values': [True,False]\n",
    "      }\n",
    "}\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict\n",
    "\n",
    "#Create the sweep\n",
    "sweep_id = wandb.sweep(sweep_config,entity=\"rucci-2053183\", project=\"Project_EAI_BrainComputerInterface\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config=None):\n",
    "  i=0\n",
    "  with wandb.init(config=config):\n",
    "    i = i + 1\n",
    "    config = wandb.config\n",
    "    if config.sparsity_loss == True and config.non_negative_constraint == True:\n",
    "      print(f\"Skipping following config becouse not supported combination sparsity_loss =>{config.sparsity_loss}, non_negative_constraint =>{config.non_negative_constraint}\")\n",
    "      print(f\"Config ==>{config}\")\n",
    "    else:\n",
    "      # bs given by the agent\n",
    "      train_dataset = DataFrameApproach1(train_df)\n",
    "      val_dataset = DataFrameApproach1(val_df)\n",
    "      test_dataset = DataFrameApproach1(test_df)\n",
    "      train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, collate_fn=collate_fn, drop_last=True)\n",
    "      val_loader = DataLoader(val_dataset, batch_size=config.batch_size,shuffle=False, collate_fn=collate_fn, drop_last=True)\n",
    "      test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, collate_fn=collate_fn, drop_last=True)\n",
    "\n",
    "      batch = next(iter(train_loader))\n",
    "      input_dim = batch[0].shape[-1]\n",
    "\n",
    "      # Model\n",
    "      model = Autoencoder(input_dim=input_dim, batch_size = config.batch_size,sparsity_factor=config.sparsity_factor ,enable_sparsity_loss=config.sparsity_loss, enable_weight_decay_loss=config.wdecay_loss, enable_non_negativity_constraint=config.non_negative_constraint, enable_wandb = True)\n",
    "      early_stop = EarlyStopping(monitor=\"val_reconstruction_loss\", mode=\"min\", patience=30, min_delta=0.001)\n",
    "\n",
    "\n",
    "      # Define the ModelCheckpoint callback to save the best model\n",
    "      checkpoint_callback = ModelCheckpoint(\n",
    "          dirpath=\"saved_models/Approach_1/\"+selected_task+\"/\",\n",
    "          filename=\"best_model\",\n",
    "          monitor=\"val_reconstruction_loss\",\n",
    "          mode=\"min\",\n",
    "          save_top_k=1,\n",
    "          save_last=True\n",
    "      )\n",
    "\n",
    "      trainer = Trainer(max_epochs=config.epochs, default_root_dir=\"saved_models/Approach_1/\", callbacks=[early_stop, checkpoint_callback],fast_dev_run=False)\n",
    "      trainer.fit(model, train_loader, val_loader)\n",
    "      trainer.test(model, test_loader)\n",
    "\n",
    "\n",
    "wandb.agent(sweep_id, train, count=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the BEST AE on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_dataset = DataFrameApproach1(train_df)\n",
    "val_dataset = DataFrameApproach1(val_df)\n",
    "test_dataset = DataFrameApproach1(test_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size,shuffle=False, collate_fn=collate_fn, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch= next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_dir = \"saved_models/Approach_1/\"+selected_task+\"/lightning_logs\"\n",
    "\n",
    "model_to_test_paths = [ ]\n",
    "\n",
    "# Add to the list the models .ckpt from the directory /saved_models/Approach_1/selected_task/\n",
    "for root, dirs, files in os.walk(\"saved_models/Approach_1/\"+selected_task):\n",
    "  for file in files:\n",
    "    if file.endswith(\".ckpt\"):\n",
    "      model_to_test_paths.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"Models to test => {model_to_test_paths}\")\n",
    "\n",
    "best_metric = 1000000000\n",
    "best_model = \"\"\n",
    "for model_path_ in model_to_test_paths:\n",
    "  model_path = model_path_\n",
    "  input_dim = batch[0].shape[-1]\n",
    "  version = model_path_.split('/')[-1]\n",
    "\n",
    "  checkpoint_model = Autoencoder(input_dim=input_dim, batch_size = batch_size,sparsity_factor=0.005,enable_sparsity_loss=False, enable_weight_decay_loss=False, enable_non_negativity_constraint=False, enable_wandb = False)\n",
    "\n",
    "  checkpoint_model.load_state_dict(torch.load(model_path, map_location=checkpoint_model.device)['state_dict']) # ------> PyTorch Lightning API\n",
    "\n",
    "  trainer = Trainer(accelerator = 'auto', fast_dev_run=False)\n",
    "  print(f\"Evaluation => {version}\")\n",
    "  trainer.test(checkpoint_model, dataloaders=test_loader)\n",
    "\n",
    "  if(checkpoint_model.test_rec_loss < best_metric):\n",
    "    best_metric = checkpoint_model.test_rec_loss\n",
    "    best_model = version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"BEST MODEL => FILE = {best_model}, MSE = {best_metric}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Model\n",
    "This is a model that make use of the z vector extracted from the autoencoder: z goes into an MLP and we discriminate between the classes.\n",
    "\n",
    "This is done for each task to study how a specific model performs over a specific tasks.\n",
    "\n",
    "The process of training the MLP allowa to backpropagate until the encoder of the AE and basically finetuning it for the classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create a mapping utility to go from label to idx and vice versa\n",
    "label2idx= {}\n",
    "idx2label = {}\n",
    "labels_task = dataset.get_dataframe()['labels'].unique()\n",
    "\n",
    "for i in range(len(labels_task)):\n",
    "  label2idx[labels_task[i]] = i\n",
    "  idx2label[str(i)] = labels_task[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierPerTask_Approach1(LightningModule):\n",
    "    def __init__(self, encoder, text_labels, task_name, enable_wandb=False):\n",
    "        super(ClassifierPerTask_Approach1, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.task_name = task_name\n",
    "        self.text_labels = text_labels\n",
    "\n",
    "        # HEAD 3\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(encoder.z_dim, 256),\n",
    "            nn.BatchNorm1d(256),  # Batch normalization\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),  # Batch normalization\n",
    "              nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, len(text_labels))\n",
    "        )\n",
    "\n",
    "        self.train_loss = []\n",
    "        self.train_accuracy = []\n",
    "        self.val_loss = []\n",
    "        self.val_accuracy = []\n",
    "        self.test_loss = []\n",
    "        self.test_accuracy = []\n",
    "\n",
    "        self.enable_wandb = enable_wandb\n",
    "\n",
    "        if self.enable_wandb:\n",
    "          wandb.init(project=\"Project_EAI_BrainComputerInterface\", entity=\"rucci-2053183\", group=\"approach1_classifier_\"+task_name)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.classifier(z)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        inputs = inputs.to(torch.float32)\n",
    "        z = self.encoder(inputs)\n",
    "        outputs = self(z)\n",
    "        labels = self.labels2TargetTensor(labels).to(torch.long)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        acc = (preds == labels).float().mean()\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('test_accuracy', acc)\n",
    "\n",
    "        self.train_loss.append(loss)\n",
    "        self.train_accuracy.append(acc)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        inputs = inputs.to(torch.float32)\n",
    "        z = self.encoder(inputs)\n",
    "        outputs = self(z)\n",
    "        labels = self.labels2TargetTensor(labels).to(torch.long)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        acc = (preds == labels).float().mean()\n",
    "        self.log('test_loss', loss)\n",
    "        self.log('test_accuracy', acc)\n",
    "\n",
    "        self.test_loss.append(loss)\n",
    "        self.test_accuracy.append(acc)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        inputs = inputs.to(torch.float32)\n",
    "        z = self.encoder(inputs)\n",
    "        outputs = self(z)\n",
    "        labels = self.labels2TargetTensor(labels).to(torch.long)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        acc = (preds == labels).float().mean()\n",
    "        self.log('val_loss', loss)\n",
    "        self.log('val_accuracy', acc)\n",
    "\n",
    "        self.val_loss.append(loss)\n",
    "        self.val_accuracy.append(acc)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "      optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
    "      scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "      return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"}\n",
    "\n",
    "    def labels2TargetTensor(self, labels):\n",
    "      target = []\n",
    "      for item in labels:\n",
    "        target.append(label2idx[item])\n",
    "\n",
    "      return torch.Tensor(target)\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        train_loss = torch.stack([x for x in self.train_loss]).mean()\n",
    "        train_acc = torch.stack([x for x in self.train_accuracy]).mean()\n",
    "\n",
    "        # Print the training loss\n",
    "        print_log = f'Training - Epoch {self.current_epoch}: Loss => {train_loss.item()} ACCURACY => {train_acc}'\n",
    "\n",
    "        self.train_loss.clear()\n",
    "        self.train_accuracy.clear()\n",
    "\n",
    "        if self.enable_wandb:\n",
    "            # Log mean training loss\n",
    "            wandb.log({\"epoch_train_loss\": train_loss, \"epoch_train_accuracy\": train_acc})\n",
    "\n",
    "        print(print_log)\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        test_loss = torch.stack([x for x in self.test_loss]).mean()\n",
    "        test_acc = torch.stack([x for x in self.test_accuracy]).mean()\n",
    "\n",
    "        # Print the training loss\n",
    "        print_log = f'Test - Epoch {self.current_epoch}: Loss => {test_loss.item()} ACCURACY => {test_acc}'\n",
    "\n",
    "        self.test_loss.clear()\n",
    "        self.test_accuracy.clear()\n",
    "\n",
    "        if self.enable_wandb:\n",
    "            # Log mean test loss and accuracy\n",
    "            wandb.log({\"epoch_test_loss\": test_loss, \"epoch_test_accuracy\": test_acc})\n",
    "\n",
    "        print(print_log)\n",
    "\n",
    "        self.test_acc = test_acc\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        val_loss = torch.stack([x for x in self.val_loss]).mean()\n",
    "        val_acc = torch.stack([x for x in self.val_accuracy]).mean()\n",
    "\n",
    "        # Print the training loss\n",
    "        print_log = f'Validation - Epoch {self.current_epoch}: Loss => {val_loss.item()} ACCURACY => {val_acc}'\n",
    "\n",
    "        self.val_loss.clear()\n",
    "        self.val_accuracy.clear()\n",
    "        self.log(\"epoch_val_accuracy\", val_acc)\n",
    "        if self.enable_wandb:\n",
    "            # Log mean validation loss and accuracy\n",
    "            wandb.log({\"epoch_val_loss\": val_loss, \"epoch_val_accuracy\": val_acc})\n",
    "            wandb.log({\"epoch\": self.current_epoch})\n",
    "\n",
    "        print(print_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Classifier âš¡ï¸âš¡ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the best AE\n",
    "base_model_dir = \"saved_models/Approach_1/\"+selected_task\n",
    "best_model_path = base_model_dir+\"/\"+best_model\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "input_dim = batch[0].shape[-1]\n",
    "checkpoint_model = Autoencoder(input_dim=input_dim, batch_size = batch_size,sparsity_factor=0.005,enable_sparsity_loss=False, enable_weight_decay_loss=False, enable_non_negativity_constraint=False, enable_wandb = False)\n",
    "checkpoint_model.load_state_dict(torch.load(best_model_path, map_location=checkpoint_model.device)['state_dict']) # ------> PyTorch Lightning API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Classifier Module for training\n",
    "encoder = checkpoint_model.encoder\n",
    "encoder.z_dim = 128\n",
    "classifier = ClassifierPerTask_Approach1(encoder, labels_task, selected_task, enable_wandb=True)\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"epoch_val_accuracy\", min_delta=0.00, patience=30, verbose=True, mode=\"max\")\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "     monitor='epoch_val_accuracy',\n",
    "     dirpath=\"saved_models/Approach_1/\"+selected_task+\"/classifier/\",\n",
    "     filename='approach1-epoch{epoch:02d}-'+selected_task,\n",
    "     auto_insert_metric_name=False,\n",
    "     mode=\"max\",\n",
    "     save_top_k=2\n",
    ")\n",
    "\n",
    "trainer_classifier = Trainer(max_epochs=100, default_root_dir=\"saved_models/Approach_1/\"+selected_task+\"/classifier/\", callbacks=[early_stop,checkpoint_callback],fast_dev_run=False)\n",
    "trainer_classifier.fit(classifier, train_loader, val_loader)\n",
    "# trainer.test(classifier, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()\n",
    "\n",
    "# Task 1 classifer wandb table \n",
    "# ancient paper task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_dataset = DataFrameApproach1(train_df)\n",
    "val_dataset = DataFrameApproach1(val_df)\n",
    "test_dataset = DataFrameApproach1(test_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size,shuffle=False, collate_fn=collate_fn, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_dir = \"saved_models/Approach_1/\"+selected_task+\"/classifier/\"\n",
    "\n",
    "model_to_test_paths = [\n",
    "    # \"approach1-epoch36-task_1.ckpt\",\n",
    "    # \"approach1-epoch57-task_1.ckpt\",\n",
    "]\n",
    "\n",
    "# Add to the list the models .ckpt from the directory /saved_models/Approach_1/selected_task/classifier/\n",
    "for root, dirs, files in os.walk(base_model_dir):\n",
    "  for file in files:\n",
    "    if file.endswith(\".ckpt\"):\n",
    "      model_to_test_paths.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"Models to test => {len(model_to_test_paths)}\")\n",
    "\n",
    "best_metric = 0\n",
    "best_model = \"\"\n",
    "for model_path_ in model_to_test_paths:\n",
    "  model_path = model_path_\n",
    "  input_dim = batch[0].shape[-1]\n",
    "  version = model_path\n",
    "\n",
    "  checkpoint_model = ClassifierPerTask_Approach1.load_from_checkpoint(model_path, enable_wandb=False)\n",
    "  \n",
    "\n",
    "  trainer = Trainer(accelerator = 'auto', fast_dev_run=False)\n",
    "  print(f\"Evaluation => {version}\")\n",
    "  trainer.test(checkpoint_model, dataloaders=test_loader)\n",
    "\n",
    "  if(checkpoint_model.test_acc > best_metric):\n",
    "    best_metric = checkpoint_model.test_acc\n",
    "    best_model = version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"BEST MODEL => FILE = {best_model}, MSE = {best_metric}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix for the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, labels, title):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt=\".2f\", cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Import the best Classifier model\n",
    "model_path = best_model\n",
    "input_dim = batch[0].shape[-1]\n",
    "checkpoint_model = ClassifierPerTask_Approach1.load_from_checkpoint(model_path, enable_wandb=False)\n",
    "checkpoint_model.eval()\n",
    "\n",
    "# Get the predictions\n",
    "y_true = []\n",
    "y_pred = []\n",
    "for batch in test_loader:\n",
    "    inputs, labels = batch\n",
    "    inputs = inputs.to(torch.float32)\n",
    "    z = checkpoint_model.encoder(inputs)\n",
    "    outputs = checkpoint_model(z)\n",
    "    preds = torch.argmax(outputs, dim=1)\n",
    "    y_true.extend(labels)\n",
    "    y_pred.extend(preds)\n",
    "\n",
    "y_true = [label2idx[item] for item in y_true]\n",
    "y_pred = [item.item() for item in y_pred]\n",
    "\n",
    "plot_confusion_matrix(y_true, y_pred, labels_task, \"Confusion Matrix - \"+selected_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 2\n",
    "**NOTE: You will find some section identical to approach 1, this is to make the running of each approach independent one from the other.**\n",
    "\n",
    "This is to address the study: \"**Observe the performance of a general model that can be used to identify states across all the tasks.**\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetApproach2(Dataset):\n",
    "  def __init__(self, window_size=1):\n",
    "    self.dataset = []\n",
    "\n",
    "    # Create a unique dataframe that is composed by the concatenation of all the files that belong to the task + the baselines files\n",
    "    tasks_name = ['task_1','task_2','task_3','task_4']\n",
    "    for task_name in tasks_name:\n",
    "      for file in dataset_task_mapping[task_name]:\n",
    "        df = pd.read_csv(file['file_path'])\n",
    "        self.dataset.append(df)\n",
    "\n",
    "    # Baseline 1\n",
    "    for file in dataset_task_mapping['baseline_1']:\n",
    "      df = pd.read_csv(file['file_path'])\n",
    "      self.dataset.append(df)\n",
    "\n",
    "    # Baseline 2\n",
    "    for file in dataset_task_mapping['baseline_2']:\n",
    "      df = pd.read_csv(file['file_path'])\n",
    "      self.dataset.append(df)\n",
    "\n",
    "    # Concatenate the dataframes\n",
    "    print(f\"Concatenating the dataframes ({len(self.dataset)})\")\n",
    "    self.dataset = pd.concat(self.dataset)\n",
    "    # Create a dataframe\n",
    "    print(f\"Dataset shape: {self.dataset.shape}\")\n",
    "\n",
    "\n",
    "    # Windowing\n",
    "    self.window_size = window_size\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.dataset) - self.window_size\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    # return as a tensor\n",
    "    print(f\"Index: {idx}\")\n",
    "    return torch.tensor(self.dataset.iloc[idx].values)\n",
    "\n",
    "  def get_dataframe(self):\n",
    "    return self.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetApproach2()\n",
    "print(f\"Dataset length: {len(dataset)}, Number of files used (Task 1: {len(dataset_task_mapping['task_1'])} + Task 2: {len(dataset_task_mapping['task_2'])} + Task 3: {len(dataset_task_mapping['task_3'])} + Task 4: {len(dataset_task_mapping['task_4'])} + Baseline 1 {len(dataset_task_mapping['baseline_1'])} + Baseline 2: {len(dataset_task_mapping['baseline_2'])})\")\n",
    "\n",
    "print(f\"Unique values of the labels: {dataset.get_dataframe()['labels'].unique()}\")\n",
    "\n",
    "# Shuffle the rows of the dataset using sklearn (making sure the shuffle is reproducible)\n",
    "from sklearn.utils import shuffle\n",
    "data = shuffle(dataset.get_dataframe(), random_state=0)\n",
    "# Remove the index column\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "# Splitting into train and test sets (80% training data, 20% testing data)\n",
    "train_df, test_df = train_test_split(data, test_size=0.15, random_state=42)\n",
    "\n",
    "# Splitting the train_df further into train and validation sets (70% training data, 30% validation data)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.15, random_state=42)\n",
    "\n",
    "print(f\"Data: {len(data)} ,Train size: {len(train_df)}, Val size: {len(val_df)}, Test size: {len(test_df)}\")\n",
    "\n",
    "\n",
    "#Create the Dataframe classe Approach 2\n",
    "class DataFrameApproach2(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe.iloc[:, :-1].values\n",
    "        self.targets = dataframe['labels'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.data[idx])\n",
    "        y = self.targets[idx]\n",
    "        return x, y\n",
    "\n",
    "def collate_fn(batch):\n",
    "    data = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "\n",
    "    # Apply min-max normalization to each column\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_data = scaler.fit_transform(data)\n",
    "\n",
    "    return torch.tensor(normalized_data), targets\n",
    "\n",
    "# Creating datasets and data loaders for each split\n",
    "train_dataset = DataFrameApproach2(train_df)\n",
    "val_dataset = DataFrameApproach2(val_df)\n",
    "test_dataset = DataFrameApproach2(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoEncoder Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, window_size = 1, enable_sparsity_loss=False):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(128),\n",
    "        )\n",
    "\n",
    "        # Apply He initialization to the linear layers\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, input_dim = x.size()  # Obtain the shape of the input [bs, input_dim]\n",
    "        input = x\n",
    "        x = self.encoder(input)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim, window_size, enable_sparsity_loss=False):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Linear(256, window_size * input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Apply He initialization to the linear layers\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = x.to(torch.float32)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AE Lightning Module âš¡ï¸âš¡ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(LightningModule):\n",
    "    def __init__(self, input_dim, batch_size, sparsity_factor=0.1, sparsity_loss_coef = 1e-3, weight_decay=0.001, window_size=window_size, enable_sparsity_loss=False, enable_weight_decay_loss=False ,enable_non_negativity_constraint=False,enable_wandb = False):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        if( enable_sparsity_loss == True and enable_non_negativity_constraint== True):\n",
    "          print(\"The combination of constraints enable_sparsity_loss and enable_non_negativity_constraint both true leads to error in to the model matrix multiplication. This will be solved by setting enable_non_negativity_constraint to False.\")\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        self.encoder = Encoder(input_dim=input_dim, window_size=window_size, enable_sparsity_loss = enable_sparsity_loss)\n",
    "        self.decoder = Decoder(input_dim=input_dim, window_size=window_size, enable_sparsity_loss = enable_sparsity_loss)\n",
    "        self.train_loss_memory = []\n",
    "        self.train_rec_loss_memory = []\n",
    "\n",
    "        self.val_loss_memory = []\n",
    "        self.val_rec_loss_memory = []\n",
    "\n",
    "        self.test_loss_memory = []\n",
    "        self.test_rec_loss_memory = []\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "\n",
    "\n",
    "        # --- Loss Settings\n",
    "        self.enable_sparsity_loss = enable_sparsity_loss\n",
    "        if enable_sparsity_loss:\n",
    "          self.sparsity_loss_coef = sparsity_loss_coef\n",
    "          self.sparsity_factor = sparsity_factor\n",
    "          print(f\"Enabled Sparsity term in the loss with sparsity loss coeff => {self.sparsity_loss_coef} and sparsity factor=>{self.sparsity_factor}\")\n",
    "\n",
    "          # self.sparsity_loss = nn.KLDivLoss(reduction='batchmean')\n",
    "          # Memory logs for sparsity\n",
    "          self.train_sparsity_loss_memory = []\n",
    "          self.val_sparsity_loss_memory = []\n",
    "          self.test_sparsity_loss_memory = []\n",
    "\n",
    "          self.enable_non_negativity_constraint = False\n",
    "        else:\n",
    "          self.enable_non_negativity_constraint = enable_non_negativity_constraint\n",
    "          if enable_non_negativity_constraint:\n",
    "            print(\"Enabled non negativity constraint\")\n",
    "\n",
    "\n",
    "        self.enable_weight_decay_loss = enable_weight_decay_loss\n",
    "        if enable_weight_decay_loss:\n",
    "          print(\"Enabled weight decay\")\n",
    "          self.weight_decay = weight_decay\n",
    "\n",
    "        self.wandb_log = enable_wandb\n",
    "\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        if torch.cuda.is_available():\n",
    "            if torch.cuda.device_count() > 1:\n",
    "                device = torch.device('cuda:0')\n",
    "                print('Using device:', device)\n",
    "            else:\n",
    "                device = torch.device('cuda')\n",
    "                print('Using device:', device)\n",
    "        else:\n",
    "            device = torch.device('cpu')\n",
    "            print('Using device:', device)\n",
    "\n",
    "\n",
    "        print('Using device:', device)\n",
    "\n",
    "        self.to(device)\n",
    "        print(f\"Initialized Model on {self.device}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "\n",
    "    def kl_div(self, p, p_hat):\n",
    "      funcs = nn.Sigmoid()\n",
    "      p_hat = torch.mean(funcs(p_hat), 1)\n",
    "      p_tensor = torch.Tensor([p] * p_hat.shape[0]).to(self.device)\n",
    "\n",
    "\n",
    "      return torch.sum(p_tensor * torch.log(p_tensor) - p_tensor * torch.log(p_hat) + (1 - p_tensor) * torch.log(1 - p_tensor) - (1 - p_tensor) * torch.log(1 - p_hat))\n",
    "\n",
    "    def sparse_loss(self, values):\n",
    "      loss = 0\n",
    "      values = values.view(self.batch_size, -1)\n",
    "\n",
    "      # Encoder sparsity\n",
    "      lyrs_encoder = list(self.encoder.encoder.children())\n",
    "      for i, lyr in enumerate(lyrs_encoder):\n",
    "          if isinstance(lyr, nn.Linear):\n",
    "            values = lyr(values)\n",
    "            # loss += self.sparsity_loss(torch.tensor([self.sparsity_factor]).to(self.device), values.to(self.device))\n",
    "            loss += self.kl_div(self.sparsity_factor, values.to(self.device))\n",
    "\n",
    "      # Decoder sparsity\n",
    "      lyrs_decoder = list(self.decoder.decoder.children())\n",
    "      for i, lyr in enumerate(lyrs_decoder):\n",
    "          if isinstance(lyr, nn.Linear):\n",
    "              values = lyr(values)\n",
    "              # loss += self.sparsity_loss(torch.tensor([self.sparsity_factor]).to(self.device), values.to(self.device))\n",
    "              loss += self.kl_div(self.sparsity_factor, values.to(self.device))\n",
    "\n",
    "      return loss\n",
    "\n",
    "    def calculate_weight_decay_loss(self):\n",
    "        weight_decay_loss = 0.0\n",
    "        for param in self.parameters():\n",
    "            weight_decay_loss += 0.5 * self.weight_decay * torch.norm(param, p=2) ** 2\n",
    "        return weight_decay_loss\n",
    "\n",
    "    def enforce_non_negativity(self):\n",
    "      for param in self.parameters():\n",
    "        param.data.clamp_(min=0, max=None)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch[0].to(torch.float32) #[bs, input_dim]\n",
    "        _, reconstructions = self(x)\n",
    "\n",
    "        x = x.view(-1) # [bs * input_dim]\n",
    "        reconstructions = reconstructions.view(-1)\n",
    "\n",
    "        loss_mse = nn.MSELoss()(reconstructions, x)\n",
    "        loss = loss_mse\n",
    "\n",
    "        if self.enable_sparsity_loss:\n",
    "          # sparsity_loss = self.sparsity_loss(torch.log(reconstructions).to(self.device), torch.tensor([self.sparsity_factor]).to(self.device))\n",
    "          sparsity_loss = self.sparse_loss(x) * self.sparsity_loss_coef\n",
    "          loss += sparsity_loss\n",
    "          self.train_sparsity_loss_memory.append(sparsity_loss)\n",
    "\n",
    "        if self.enable_weight_decay_loss:\n",
    "          weight_decay_loss = self.calculate_weight_decay_loss()\n",
    "          loss += weight_decay_loss\n",
    "\n",
    "        self.train_loss_memory.append(loss)\n",
    "        self.train_rec_loss_memory.append(loss_mse)\n",
    "\n",
    "        if self.wandb_log:\n",
    "          wandb.log({\"train_total_loss\": loss})\n",
    "          wandb.log({\"train_reconstruction_loss\": loss_mse})\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "      x = batch[0].to(torch.float32)\n",
    "      _, reconstructions = self(x)\n",
    "\n",
    "      x = x.view(-1) #[]\n",
    "      reconstructions = reconstructions.view(-1)\n",
    "\n",
    "      loss_mse = nn.MSELoss()(reconstructions, x)\n",
    "      loss = loss_mse\n",
    "\n",
    "      if self.enable_sparsity_loss:\n",
    "        # sparsity_loss = self.sparsity_loss(torch.log(reconstructions).to(self.device), torch.tensor([self.sparsity_factor]).to(self.device))\n",
    "        sparsity_loss = self.sparse_loss(x) * self.sparsity_loss_coef\n",
    "        loss += sparsity_loss\n",
    "        self.val_sparsity_loss_memory.append(sparsity_loss)\n",
    "\n",
    "      if self.enable_weight_decay_loss:\n",
    "        weight_decay_loss = self.calculate_weight_decay_loss()\n",
    "        loss += weight_decay_loss\n",
    "\n",
    "      if self.enable_non_negativity_constraint:\n",
    "        self.enforce_non_negativity()\n",
    "\n",
    "      self.val_loss_memory.append(loss)\n",
    "      self.val_rec_loss_memory.append(loss_mse)\n",
    "\n",
    "      if self.wandb_log:\n",
    "        wandb.log({\"val_total_loss\": loss})\n",
    "        wandb.log({\"val_reconstruction_loss\": loss_mse})\n",
    "\n",
    "      # For early stop and Model checkpoint callbacks\n",
    "      self.log(\"val_reconstruction_loss\",loss_mse)\n",
    "\n",
    "      return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "      x = batch[0].to(torch.float32)\n",
    "      _, reconstructions = self(x)\n",
    "\n",
    "      x = x.view(-1) #[]\n",
    "      reconstructions = reconstructions.view(-1)\n",
    "\n",
    "      loss_mse = nn.MSELoss()(reconstructions, x)\n",
    "      loss = loss_mse\n",
    "\n",
    "      if self.enable_sparsity_loss:\n",
    "        # sparsity_loss = self.sparsity_loss(torch.log(reconstructions).to(self.device), torch.tensor([self.sparsity_factor]).to(self.device))\n",
    "        sparsity_loss = self.sparse_loss(x) * self.sparsity_loss_coef\n",
    "        loss += sparsity_loss\n",
    "        self.test_sparsity_loss_memory.append(sparsity_loss)\n",
    "\n",
    "      if self.enable_weight_decay_loss:\n",
    "        weight_decay_loss = self.calculate_weight_decay_loss()\n",
    "        loss += weight_decay_loss\n",
    "\n",
    "      if self.enable_non_negativity_constraint:\n",
    "        self.enforce_non_negativity()\n",
    "\n",
    "      self.test_loss_memory.append(loss)\n",
    "      self.test_rec_loss_memory.append(loss_mse)\n",
    "\n",
    "\n",
    "      return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=10)  # Adjust T_max as needed\n",
    "\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': {'scheduler': scheduler, 'interval': 'epoch'}}\n",
    "\n",
    "    def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_closure):\n",
    "        # step\n",
    "        optimizer.step(closure=optimizer_closure)\n",
    "\n",
    "        if self.enable_non_negativity_constraint:\n",
    "          self.enforce_non_negativity()\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.wandb_log:\n",
    "            wandb.log({'epoch': self.current_epoch})\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        # Access the training loss from the outputs\n",
    "        train_loss = torch.stack([x for x in self.train_loss_memory]).mean()\n",
    "        train_rec_loss = torch.stack([x for x in self.train_rec_loss_memory]).mean()\n",
    "\n",
    "        # Print the training loss\n",
    "        print_log = f'Training Loss - Epoch {self.current_epoch}: Total Loss => {train_loss.item()} MSE => {train_rec_loss}'\n",
    "\n",
    "        self.train_loss_memory.clear()\n",
    "        self.train_rec_loss_memory.clear()\n",
    "\n",
    "        if self.enable_sparsity_loss:\n",
    "          train_sparsity_loss = torch.stack([x for x in self.train_sparsity_loss_memory]).mean()\n",
    "          print_log += f' SPARSE => {train_sparsity_loss}'\n",
    "          self.train_sparsity_loss_memory.clear()\n",
    "\n",
    "        if self.wandb_log:\n",
    "          wandb.log({\"train_total_loss\": train_loss})\n",
    "          wandb.log({\"train_reconstruction_loss\": train_rec_loss})\n",
    "          if self.enable_sparsity_loss:\n",
    "            wandb.log({\"train_sparse_loss\": train_sparsity_loss})\n",
    "\n",
    "        print(print_log)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        # Access the training loss from the outputs\n",
    "        val_loss = torch.stack([x for x in self.val_loss_memory]).mean()\n",
    "        val_rec_loss = torch.stack([x for x in self.val_rec_loss_memory]).mean()\n",
    "\n",
    "        # Print the training loss\n",
    "        print_log = f'Validation Loss - Epoch {self.current_epoch}: Total Loss => {val_loss.item()} MSE => {val_rec_loss}'\n",
    "\n",
    "        self.val_loss_memory.clear()\n",
    "        self.val_rec_loss_memory.clear()\n",
    "\n",
    "        if self.enable_sparsity_loss:\n",
    "          val_sparsity_loss = torch.stack([x for x in self.val_sparsity_loss_memory]).mean()\n",
    "          print_log += f' SPARSE => {val_sparsity_loss}'\n",
    "          self.val_sparsity_loss_memory.clear()\n",
    "\n",
    "        if self.wandb_log:\n",
    "          wandb.log({\"val_total_loss\": val_loss})\n",
    "          wandb.log({\"val_reconstruction_loss\": val_rec_loss})\n",
    "          if self.enable_sparsity_loss:\n",
    "            wandb.log({\"val_sparse_loss\": val_sparsity_loss})\n",
    "\n",
    "        print(print_log)\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        # Access the training loss from the outputs\n",
    "        test_loss = torch.stack([x for x in self.test_loss_memory]).mean()\n",
    "        test_rec_loss = torch.stack([x for x in self.test_rec_loss_memory]).mean()\n",
    "\n",
    "        # Print the training loss\n",
    "        print_log = f'Test Loss - Epoch {self.current_epoch}: Total Loss => {test_loss.item()} MSE => {test_rec_loss}'\n",
    "\n",
    "        self.test_loss_memory.clear()\n",
    "        self.test_rec_loss_memory.clear()\n",
    "\n",
    "        if self.enable_sparsity_loss:\n",
    "          test_sparsity_loss = torch.stack([x for x in self.test_sparsity_loss_memory]).mean()\n",
    "          print_log += f' SPARSE => {test_sparsity_loss}'\n",
    "          self.test_sparsity_loss_memory.clear()\n",
    "\n",
    "        if self.wandb_log:\n",
    "          wandb.log({\"test_total_loss\": test_loss})\n",
    "          wandb.log({\"test_reconstruction_loss\": test_rec_loss})\n",
    "          if self.enable_sparsity_loss:\n",
    "            wandb.log({\"test_sparse_loss\": test_sparsity_loss})\n",
    "\n",
    "        self.test_rec_loss = test_rec_loss\n",
    "\n",
    "        print(print_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Model\n",
    "This is a model that make use of the z vector extracted from the autoencoder: z goes into an MLP and we discriminate between the classes now across all the tasks.\n",
    "\n",
    "The process of training the MLP allows to backpropagate until the encoder of the AE and basilly finetuning it for the classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create a mapping utility to go from label to idx and vice versa\n",
    "label2idx= {}\n",
    "idx2label = {}\n",
    "labels_task = dataset.get_dataframe()['labels'].unique()\n",
    "\n",
    "for i in range(len(labels_task)):\n",
    "  label2idx[labels_task[i]] = i\n",
    "  idx2label[str(i)] = labels_task[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierPerTask_Approach2(LightningModule):\n",
    "    def __init__(self, encoder, text_labels, head_type=1, enable_wandb=False):\n",
    "        super(ClassifierPerTask_Approach2, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.text_labels = text_labels\n",
    "        if(head_type==1):\n",
    "          # HEAD 1\n",
    "          self.classifier = nn.Sequential(\n",
    "              nn.Linear(encoder.z_dim, 128),\n",
    "              nn.ReLU(),\n",
    "              nn.Linear(128, len(text_labels))\n",
    "          )\n",
    "        elif (head_type ==2):\n",
    "          # HEAD 2\n",
    "          self.classifier = nn.Sequential(\n",
    "              nn.Linear(encoder.z_dim, 256),\n",
    "              nn.ReLU(),\n",
    "               nn.Dropout(0.2),\n",
    "              nn.Linear(256, 128),\n",
    "              nn.ReLU(),\n",
    "              nn.Dropout(0.2),\n",
    "              nn.Linear(128, len(text_labels))\n",
    "          )\n",
    "        elif (head_type ==3):\n",
    "          # HEAD 3\n",
    "          self.classifier = nn.Sequential(\n",
    "              nn.Linear(encoder.z_dim, 256),\n",
    "              nn.BatchNorm1d(256),  # Batch normalization\n",
    "              nn.ReLU(),\n",
    "              nn.Dropout(0.2),\n",
    "              nn.Linear(256, 128),\n",
    "              nn.BatchNorm1d(128),  # Batch normalization\n",
    "               nn.ReLU(),\n",
    "              nn.Dropout(0.2),\n",
    "              nn.Linear(128, len(text_labels))\n",
    "          )\n",
    "        else:\n",
    "          # HEAD 4\n",
    "          self.classifier = nn.Sequential(\n",
    "              nn.Linear(encoder.z_dim, 256),\n",
    "              nn.LayerNorm(256),  # Apply layer normalization\n",
    "              nn.ReLU(),\n",
    "              nn.Dropout(0.2),\n",
    "              nn.Linear(256, 128),\n",
    "              nn.LayerNorm(128),  # Apply layer normalization\n",
    "              nn.ReLU(),\n",
    "              nn.Dropout(0.2),\n",
    "              nn.Linear(128, len(text_labels))\n",
    "          )\n",
    "\n",
    "        self.train_loss = []\n",
    "        self.train_accuracy = []\n",
    "        self.val_loss = []\n",
    "        self.val_accuracy = []\n",
    "        self.test_loss = []\n",
    "        self.test_accuracy = []\n",
    "\n",
    "        self.enable_wandb = enable_wandb\n",
    "\n",
    "        if self.enable_wandb:\n",
    "          wandb.init(project=\"Project_EAI_BrainComputerInterface\", entity=\"rucci-2053183\", group=\"approach2_classifier\")\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.classifier(z)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        inputs = inputs.to(torch.float32)\n",
    "        z = self.encoder(inputs)\n",
    "        outputs = self(z)\n",
    "        labels = self.labels2TargetTensor(labels).to(torch.long).to(outputs.device)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        acc = (preds == labels).float().mean()\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('test_accuracy', acc)\n",
    "\n",
    "        self.train_loss.append(loss)\n",
    "        self.train_accuracy.append(acc)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        inputs = inputs.to(torch.float32)\n",
    "        z = self.encoder(inputs)\n",
    "        outputs = self(z)\n",
    "        labels = self.labels2TargetTensor(labels).to(torch.long).to(outputs.device)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        acc = (preds == labels).float().mean()\n",
    "        self.log('test_loss', loss)\n",
    "        self.log('test_accuracy', acc)\n",
    "\n",
    "        self.test_loss.append(loss)\n",
    "        self.test_accuracy.append(acc)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        inputs = inputs.to(torch.float32)\n",
    "        z = self.encoder(inputs)\n",
    "        outputs = self(z)\n",
    "        labels = self.labels2TargetTensor(labels).to(torch.long).to(outputs.device)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        acc = (preds == labels).float().mean()\n",
    "        self.log('val_loss', loss)\n",
    "        self.log('val_accuracy', acc)\n",
    "\n",
    "        self.val_loss.append(loss)\n",
    "        self.val_accuracy.append(acc)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "      optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
    "      scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "      return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"}\n",
    "\n",
    "    def labels2TargetTensor(self, labels):\n",
    "      target = []\n",
    "      for item in labels:\n",
    "        target.append(label2idx[item])\n",
    "\n",
    "      return torch.Tensor(target)\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        train_loss = torch.stack([x for x in self.train_loss]).mean()\n",
    "        train_acc = torch.stack([x for x in self.train_accuracy]).mean()\n",
    "\n",
    "        # Print the training loss\n",
    "        print_log = f'Training - Epoch {self.current_epoch}: Loss => {train_loss.item()} ACCURACY => {train_acc}'\n",
    "\n",
    "        self.train_loss.clear()\n",
    "        self.train_accuracy.clear()\n",
    "\n",
    "        if self.enable_wandb:\n",
    "            # Log mean training loss\n",
    "            wandb.log({\"epoch_train_loss\": train_loss, \"epoch_train_accuracy\": train_acc})\n",
    "\n",
    "        print(print_log)\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        test_loss = torch.stack([x for x in self.test_loss]).mean()\n",
    "        test_acc = torch.stack([x for x in self.test_accuracy]).mean()\n",
    "\n",
    "        # Print the training loss\n",
    "        print_log = f'Test - Epoch {self.current_epoch}: Loss => {test_loss.item()} ACCURACY => {test_acc}'\n",
    "\n",
    "        self.test_loss.clear()\n",
    "        self.test_accuracy.clear()\n",
    "\n",
    "        if self.enable_wandb:\n",
    "            # Log mean test loss and accuracy\n",
    "            wandb.log({\"epoch_test_loss\": test_loss, \"epoch_test_accuracy\": test_acc})\n",
    "\n",
    "        print(print_log)\n",
    "\n",
    "        self.test_acc = test_acc\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        val_loss = torch.stack([x for x in self.val_loss]).mean()\n",
    "        val_acc = torch.stack([x for x in self.val_accuracy]).mean()\n",
    "\n",
    "        # Print the training loss\n",
    "        print_log = f'Validation - Epoch {self.current_epoch}: Loss => {val_loss.item()} ACCURACY => {val_acc}'\n",
    "\n",
    "        self.val_loss.clear()\n",
    "        self.val_accuracy.clear()\n",
    "        self.log(\"epoch_val_accuracy\", val_acc)\n",
    "        if self.enable_wandb:\n",
    "            # Log mean validation loss and accuracy\n",
    "            wandb.log({\"epoch_val_loss\": val_loss, \"epoch_val_accuracy\": val_acc})\n",
    "            wandb.log({\"epoch\": self.current_epoch})\n",
    "\n",
    "        print(print_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_dataset = DataFrameApproach2(train_df)\n",
    "val_dataset = DataFrameApproach2(val_df)\n",
    "test_dataset = DataFrameApproach2(test_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size,shuffle=False, collate_fn=collate_fn, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import The best classifier model\n",
    "base_model_dir = \"saved_models/Approach_2/classifier/head_3\"\n",
    "\n",
    "model_to_test_paths = [\n",
    "    # \"approach1-epoch36-task_1.ckpt\",\n",
    "    # \"approach1-epoch57-task_1.ckpt\",\n",
    "]\n",
    "\n",
    "# Add to the list the models .ckpt from the directory /saved_models/Approach_1/selected_task/classifier/\n",
    "for root, dirs, files in os.walk(base_model_dir):\n",
    "  for file in files:\n",
    "    if file.endswith(\".ckpt\"):\n",
    "      model_to_test_paths.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"Models to test => {len(model_to_test_paths)}\")\n",
    "\n",
    "best_metric = 0\n",
    "best_model = \"\"\n",
    "for model_path_ in model_to_test_paths:\n",
    "  model_path = model_path_\n",
    "  input_dim = batch[0].shape[-1]\n",
    "  version = model_path\n",
    "\n",
    "  checkpoint_model = ClassifierPerTask_Approach2.load_from_checkpoint(model_path, head_type=3, enable_wandb=False)\n",
    "  \n",
    "\n",
    "  trainer = Trainer(accelerator = 'auto', fast_dev_run=False)\n",
    "  print(f\"Evaluation => {version}\")\n",
    "  trainer.test(checkpoint_model, dataloaders=test_loader)\n",
    "\n",
    "  if(checkpoint_model.test_acc > best_metric):\n",
    "    best_metric = checkpoint_model.test_acc\n",
    "    best_model = version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix for the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, labels, title):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt=\".2f\", cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Import the best Classifier model\n",
    "model_path = best_model\n",
    "input_dim = batch[0].shape[-1]\n",
    "checkpoint_model = ClassifierPerTask_Approach2.load_from_checkpoint(model_path, head_type=3, enable_wandb=False)\n",
    "checkpoint_model.eval()\n",
    "\n",
    "# Get the predictions\n",
    "y_true = []\n",
    "y_pred = []\n",
    "for batch in test_loader:\n",
    "    inputs, labels = batch\n",
    "    inputs = inputs.to(torch.float32)\n",
    "    z = checkpoint_model.encoder(inputs)\n",
    "    outputs = checkpoint_model(z)\n",
    "    preds = torch.argmax(outputs, dim=1)\n",
    "    y_true.extend(labels)\n",
    "    y_pred.extend(preds)\n",
    "\n",
    "y_true = [label2idx[item] for item in y_true]\n",
    "y_pred = [item.item() for item in y_pred]\n",
    "\n",
    "plot_confusion_matrix(y_true, y_pred, labels_task, \"Confusion Matrix - General Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 4\n",
    "Might be interesting to investigate how much a contrastive finetuning/training of the autoencoder might affect the performance at approach 3.\n",
    "This comes from the fact that the general autoencoder reconstruction task, doesn't take into account the objective of discriminate zones of the latent space according to the classes.\n",
    "\n",
    "This also allows to make a study in which an encoder is trained from scratch in the same settings, with the same idea of use the Z representation to downstream classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetApproach4(Dataset):\n",
    "  def __init__(self, window_size=1):\n",
    "    self.dataset = []\n",
    "\n",
    "    # Create a unique dataframe that is composed by the concatenation of all the files that belong to the task + the baselines files\n",
    "    tasks_name = ['task_1','task_2','task_3','task_4']\n",
    "    for task_name in tasks_name:\n",
    "      for file in dataset_task_mapping[task_name]:\n",
    "        df = pd.read_csv(file['file_path'])\n",
    "        self.dataset.append(df)\n",
    "\n",
    "    # Baseline 1\n",
    "    for file in dataset_task_mapping['baseline_1']:\n",
    "      df = pd.read_csv(file['file_path'])\n",
    "      self.dataset.append(df)\n",
    "\n",
    "    # Baseline 2\n",
    "    for file in dataset_task_mapping['baseline_2']:\n",
    "      df = pd.read_csv(file['file_path'])\n",
    "      self.dataset.append(df)\n",
    "\n",
    "    # Concatenate the dataframes\n",
    "    print(f\"Concatenating the dataframes ({len(self.dataset)})\")\n",
    "    self.dataset = pd.concat(self.dataset)\n",
    "    # Create a dataframe\n",
    "    print(f\"Dataset shape: {self.dataset.shape}\")\n",
    "\n",
    "\n",
    "    # Windowing\n",
    "    self.window_size = window_size\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.dataset) - self.window_size\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    # return as a tensor\n",
    "    print(f\"Index: {idx}\")\n",
    "    return torch.tensor(self.dataset.iloc[idx].values)\n",
    "\n",
    "  def get_dataframe(self):\n",
    "    return self.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating the dataframes (27)\n",
      "Dataset shape: (12477, 65)\n",
      "Dataset length: 12476, Number of files used (Task 1: 10 + Task 2: 3 + Task 3: 5 + Task 4: 2 + Baseline 1 4 + Baseline 2: 3)\n",
      "Unique values of the labels: ['TASK1T0' 'TASK1T2' 'TASK1T1' 'TASK2T0' 'TASK2T2' 'TASK2T1' 'TASK3T0'\n",
      " 'TASK3T2' 'TASK3T1' 'TASK4T0' 'TASK4T1' 'TASK4T2' 'BASE1T0' 'BASE2T0']\n",
      "Data: 12477 ,Train size: 9014, Val size: 1591, Test size: 1872\n"
     ]
    }
   ],
   "source": [
    "dataset = DatasetApproach4()\n",
    "print(f\"Dataset length: {len(dataset)}, Number of files used (Task 1: {len(dataset_task_mapping['task_1'])} + Task 2: {len(dataset_task_mapping['task_2'])} + Task 3: {len(dataset_task_mapping['task_3'])} + Task 4: {len(dataset_task_mapping['task_4'])} + Baseline 1 {len(dataset_task_mapping['baseline_1'])} + Baseline 2: {len(dataset_task_mapping['baseline_2'])})\")\n",
    "print(f\"Unique values of the labels: {dataset.get_dataframe()['labels'].unique()}\")\n",
    "dataset_labels = dataset.get_dataframe()['labels'].unique().tolist()\n",
    "\n",
    "# Shuffle the rows of the dataset using sklearn (making sure the shuffle is reproducible)\n",
    "from sklearn.utils import shuffle\n",
    "data = shuffle(dataset.get_dataframe(), random_state=0)\n",
    "# Remove the index column\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "# Splitting into train and test sets (80% training data, 20% testing data)\n",
    "train_df, test_df = train_test_split(data, test_size=0.15, random_state=42)\n",
    "\n",
    "# Splitting the train_df further into train and validation sets (70% training data, 30% validation data)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.15, random_state=42)\n",
    "\n",
    "print(f\"Data: {len(data)} ,Train size: {len(train_df)}, Val size: {len(val_df)}, Test size: {len(test_df)}\")\n",
    "\n",
    "#Create the Dataframe class Approach 3\n",
    "class DataFrameApproach4(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe.iloc[:, :-1].values\n",
    "        self.targets = dataframe['labels'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.data[idx])\n",
    "        y = self.targets[idx]\n",
    "\n",
    "        return x, y\n",
    "\n",
    "def collate_fn(batch):\n",
    "    data = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "\n",
    "    # Apply min-max normalization to each column\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_data = scaler.fit_transform(data)\n",
    "\n",
    "    return torch.tensor(normalized_data), targets\n",
    "\n",
    "# Creating datasets and data loaders for each split\n",
    "train_dataset = DataFrameApproach4(train_df)\n",
    "val_dataset = DataFrameApproach4(val_df)\n",
    "test_dataset = DataFrameApproach4(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoEncoder Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, window_size = 1, enable_sparsity_loss=False):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(128),\n",
    "        )\n",
    "\n",
    "        # Apply He initialization to the linear layers\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, input_dim = x.size()  # Obtain the shape of the input [bs, input_dim]\n",
    "        input = x\n",
    "        x = self.encoder(input)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim, window_size, enable_sparsity_loss=False):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Linear(256, window_size * input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Apply He initialization to the linear layers\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = x.to(torch.float32)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AE Lightning Module âš¡ï¸âš¡ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(LightningModule):\n",
    "    def __init__(self, input_dim, batch_size, sparsity_factor=0.1, sparsity_loss_coef = 1e-3, weight_decay=0.001, window_size=window_size, enable_sparsity_loss=False, enable_weight_decay_loss=False ,enable_non_negativity_constraint=False,enable_wandb = False, decoder_none=False):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        if( enable_sparsity_loss == True and enable_non_negativity_constraint== True):\n",
    "          print(\"The combination of constraints enable_sparsity_loss and enable_non_negativity_constraint both true leads to error in to the model matrix multiplication. This will be solved by setting enable_non_negativity_constraint to False.\")\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        self.encoder = Encoder(input_dim=input_dim, window_size=window_size, enable_sparsity_loss = enable_sparsity_loss)\n",
    "        if not decoder_none:\n",
    "          self.decoder = Decoder(input_dim=input_dim, window_size=window_size, enable_sparsity_loss = enable_sparsity_loss)\n",
    "        self.train_loss_memory = []\n",
    "        self.train_rec_loss_memory = []\n",
    "\n",
    "        self.val_loss_memory = []\n",
    "        self.val_rec_loss_memory = []\n",
    "\n",
    "        self.test_loss_memory = []\n",
    "        self.test_rec_loss_memory = []\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "\n",
    "\n",
    "        # --- Loss Settings\n",
    "        self.enable_sparsity_loss = enable_sparsity_loss\n",
    "        if enable_sparsity_loss:\n",
    "          self.sparsity_loss_coef = sparsity_loss_coef\n",
    "          self.sparsity_factor = sparsity_factor\n",
    "          print(f\"Enabled Sparsity term in the loss with sparsity loss coeff => {self.sparsity_loss_coef} and sparsity factor=>{self.sparsity_factor}\")\n",
    "\n",
    "          # self.sparsity_loss = nn.KLDivLoss(reduction='batchmean')\n",
    "          # Memory logs for sparsity\n",
    "          self.train_sparsity_loss_memory = []\n",
    "          self.val_sparsity_loss_memory = []\n",
    "          self.test_sparsity_loss_memory = []\n",
    "\n",
    "          self.enable_non_negativity_constraint = False\n",
    "        else:\n",
    "          self.enable_non_negativity_constraint = enable_non_negativity_constraint\n",
    "          if enable_non_negativity_constraint:\n",
    "            print(\"Enabled non negativity constraint\")\n",
    "\n",
    "\n",
    "        self.enable_weight_decay_loss = enable_weight_decay_loss\n",
    "        if enable_weight_decay_loss:\n",
    "          print(\"Enabled weight decay\")\n",
    "          self.weight_decay = weight_decay\n",
    "\n",
    "        self.wandb_log = enable_wandb\n",
    "\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        if torch.cuda.is_available():\n",
    "            if torch.cuda.device_count() > 1:\n",
    "                device = torch.device('cuda:0')\n",
    "                print('Using device:', device)\n",
    "            else:\n",
    "                device = torch.device('cuda')\n",
    "                print('Using device:', device)\n",
    "        else:\n",
    "            device = torch.device('cpu')\n",
    "            print('Using device:', device)\n",
    "\n",
    "\n",
    "        print('Using device:', device)\n",
    "\n",
    "        self.to(device)\n",
    "        print(f\"Initialized Model on {self.device}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "\n",
    "    def kl_div(self, p, p_hat):\n",
    "      funcs = nn.Sigmoid()\n",
    "      p_hat = torch.mean(funcs(p_hat), 1)\n",
    "      p_tensor = torch.Tensor([p] * p_hat.shape[0]).to(self.device)\n",
    "\n",
    "\n",
    "      return torch.sum(p_tensor * torch.log(p_tensor) - p_tensor * torch.log(p_hat) + (1 - p_tensor) * torch.log(1 - p_tensor) - (1 - p_tensor) * torch.log(1 - p_hat))\n",
    "\n",
    "    def sparse_loss(self, values):\n",
    "      loss = 0\n",
    "      values = values.view(self.batch_size, -1)\n",
    "\n",
    "      # Encoder sparsity\n",
    "      lyrs_encoder = list(self.encoder.encoder.children())\n",
    "      for i, lyr in enumerate(lyrs_encoder):\n",
    "          if isinstance(lyr, nn.Linear):\n",
    "            values = lyr(values)\n",
    "            # loss += self.sparsity_loss(torch.tensor([self.sparsity_factor]).to(self.device), values.to(self.device))\n",
    "            loss += self.kl_div(self.sparsity_factor, values.to(self.device))\n",
    "\n",
    "      # Decoder sparsity\n",
    "      lyrs_decoder = list(self.decoder.decoder.children())\n",
    "      for i, lyr in enumerate(lyrs_decoder):\n",
    "          if isinstance(lyr, nn.Linear):\n",
    "              values = lyr(values)\n",
    "              # loss += self.sparsity_loss(torch.tensor([self.sparsity_factor]).to(self.device), values.to(self.device))\n",
    "              loss += self.kl_div(self.sparsity_factor, values.to(self.device))\n",
    "\n",
    "      return loss\n",
    "\n",
    "    def calculate_weight_decay_loss(self):\n",
    "        weight_decay_loss = 0.0\n",
    "        for param in self.parameters():\n",
    "            weight_decay_loss += 0.5 * self.weight_decay * torch.norm(param, p=2) ** 2\n",
    "        return weight_decay_loss\n",
    "\n",
    "    def enforce_non_negativity(self):\n",
    "      for param in self.parameters():\n",
    "        param.data.clamp_(min=0, max=None)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch[0].to(torch.float32) #[bs, input_dim]\n",
    "        _, reconstructions = self(x)\n",
    "\n",
    "        x = x.view(-1) # [bs * input_dim]\n",
    "        reconstructions = reconstructions.view(-1)\n",
    "\n",
    "        loss_mse = nn.MSELoss()(reconstructions, x)\n",
    "        loss = loss_mse\n",
    "\n",
    "        if self.enable_sparsity_loss:\n",
    "          # sparsity_loss = self.sparsity_loss(torch.log(reconstructions).to(self.device), torch.tensor([self.sparsity_factor]).to(self.device))\n",
    "          sparsity_loss = self.sparse_loss(x) * self.sparsity_loss_coef\n",
    "          loss += sparsity_loss\n",
    "          self.train_sparsity_loss_memory.append(sparsity_loss)\n",
    "\n",
    "        if self.enable_weight_decay_loss:\n",
    "          weight_decay_loss = self.calculate_weight_decay_loss()\n",
    "          loss += weight_decay_loss\n",
    "\n",
    "        self.train_loss_memory.append(loss)\n",
    "        self.train_rec_loss_memory.append(loss_mse)\n",
    "\n",
    "        if self.wandb_log:\n",
    "          wandb.log({\"train_total_loss\": loss})\n",
    "          wandb.log({\"train_reconstruction_loss\": loss_mse})\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "      x = batch[0].to(torch.float32)\n",
    "      _, reconstructions = self(x)\n",
    "\n",
    "      x = x.view(-1) #[]\n",
    "      reconstructions = reconstructions.view(-1)\n",
    "\n",
    "      loss_mse = nn.MSELoss()(reconstructions, x)\n",
    "      loss = loss_mse\n",
    "\n",
    "      if self.enable_sparsity_loss:\n",
    "        # sparsity_loss = self.sparsity_loss(torch.log(reconstructions).to(self.device), torch.tensor([self.sparsity_factor]).to(self.device))\n",
    "        sparsity_loss = self.sparse_loss(x) * self.sparsity_loss_coef\n",
    "        loss += sparsity_loss\n",
    "        self.val_sparsity_loss_memory.append(sparsity_loss)\n",
    "\n",
    "      if self.enable_weight_decay_loss:\n",
    "        weight_decay_loss = self.calculate_weight_decay_loss()\n",
    "        loss += weight_decay_loss\n",
    "\n",
    "      if self.enable_non_negativity_constraint:\n",
    "        self.enforce_non_negativity()\n",
    "\n",
    "      self.val_loss_memory.append(loss)\n",
    "      self.val_rec_loss_memory.append(loss_mse)\n",
    "\n",
    "      if self.wandb_log:\n",
    "        wandb.log({\"val_total_loss\": loss})\n",
    "        wandb.log({\"val_reconstruction_loss\": loss_mse})\n",
    "\n",
    "      # For early stop and Model checkpoint callbacks\n",
    "      self.log(\"val_reconstruction_loss\",loss_mse)\n",
    "\n",
    "      return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "      x = batch[0].to(torch.float32)\n",
    "      _, reconstructions = self(x)\n",
    "\n",
    "      x = x.view(-1) #[]\n",
    "      reconstructions = reconstructions.view(-1)\n",
    "\n",
    "      loss_mse = nn.MSELoss()(reconstructions, x)\n",
    "      loss = loss_mse\n",
    "\n",
    "      if self.enable_sparsity_loss:\n",
    "        # sparsity_loss = self.sparsity_loss(torch.log(reconstructions).to(self.device), torch.tensor([self.sparsity_factor]).to(self.device))\n",
    "        sparsity_loss = self.sparse_loss(x) * self.sparsity_loss_coef\n",
    "        loss += sparsity_loss\n",
    "        self.test_sparsity_loss_memory.append(sparsity_loss)\n",
    "\n",
    "      if self.enable_weight_decay_loss:\n",
    "        weight_decay_loss = self.calculate_weight_decay_loss()\n",
    "        loss += weight_decay_loss\n",
    "\n",
    "      if self.enable_non_negativity_constraint:\n",
    "        self.enforce_non_negativity()\n",
    "\n",
    "      self.test_loss_memory.append(loss)\n",
    "      self.test_rec_loss_memory.append(loss_mse)\n",
    "\n",
    "\n",
    "      return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=10)  # Adjust T_max as needed\n",
    "\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': {'scheduler': scheduler, 'interval': 'epoch'}}\n",
    "\n",
    "    def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_closure):\n",
    "        # step\n",
    "        optimizer.step(closure=optimizer_closure)\n",
    "\n",
    "        if self.enable_non_negativity_constraint:\n",
    "          self.enforce_non_negativity()\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.wandb_log:\n",
    "            wandb.log({'epoch': self.current_epoch})\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        # Access the training loss from the outputs\n",
    "        train_loss = torch.stack([x for x in self.train_loss_memory]).mean()\n",
    "        train_rec_loss = torch.stack([x for x in self.train_rec_loss_memory]).mean()\n",
    "\n",
    "        # Print the training loss\n",
    "        print_log = f'Training Loss - Epoch {self.current_epoch}: Total Loss => {train_loss.item()} MSE => {train_rec_loss}'\n",
    "\n",
    "        self.train_loss_memory.clear()\n",
    "        self.train_rec_loss_memory.clear()\n",
    "\n",
    "        if self.enable_sparsity_loss:\n",
    "          train_sparsity_loss = torch.stack([x for x in self.train_sparsity_loss_memory]).mean()\n",
    "          print_log += f' SPARSE => {train_sparsity_loss}'\n",
    "          self.train_sparsity_loss_memory.clear()\n",
    "\n",
    "        if self.wandb_log:\n",
    "          wandb.log({\"train_total_loss\": train_loss})\n",
    "          wandb.log({\"train_reconstruction_loss\": train_rec_loss})\n",
    "          if self.enable_sparsity_loss:\n",
    "            wandb.log({\"train_sparse_loss\": train_sparsity_loss})\n",
    "\n",
    "        print(print_log)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        # Access the training loss from the outputs\n",
    "        val_loss = torch.stack([x for x in self.val_loss_memory]).mean()\n",
    "        val_rec_loss = torch.stack([x for x in self.val_rec_loss_memory]).mean()\n",
    "\n",
    "        # Print the training loss\n",
    "        print_log = f'Validation Loss - Epoch {self.current_epoch}: Total Loss => {val_loss.item()} MSE => {val_rec_loss}'\n",
    "\n",
    "        self.val_loss_memory.clear()\n",
    "        self.val_rec_loss_memory.clear()\n",
    "\n",
    "        if self.enable_sparsity_loss:\n",
    "          val_sparsity_loss = torch.stack([x for x in self.val_sparsity_loss_memory]).mean()\n",
    "          print_log += f' SPARSE => {val_sparsity_loss}'\n",
    "          self.val_sparsity_loss_memory.clear()\n",
    "\n",
    "        if self.wandb_log:\n",
    "          wandb.log({\"val_total_loss\": val_loss})\n",
    "          wandb.log({\"val_reconstruction_loss\": val_rec_loss})\n",
    "          if self.enable_sparsity_loss:\n",
    "            wandb.log({\"val_sparse_loss\": val_sparsity_loss})\n",
    "\n",
    "        print(print_log)\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        # Access the training loss from the outputs\n",
    "        test_loss = torch.stack([x for x in self.test_loss_memory]).mean()\n",
    "        test_rec_loss = torch.stack([x for x in self.test_rec_loss_memory]).mean()\n",
    "\n",
    "        # Print the training loss\n",
    "        print_log = f'Test Loss - Epoch {self.current_epoch}: Total Loss => {test_loss.item()} MSE => {test_rec_loss}'\n",
    "\n",
    "        self.test_loss_memory.clear()\n",
    "        self.test_rec_loss_memory.clear()\n",
    "\n",
    "        if self.enable_sparsity_loss:\n",
    "          test_sparsity_loss = torch.stack([x for x in self.test_sparsity_loss_memory]).mean()\n",
    "          print_log += f' SPARSE => {test_sparsity_loss}'\n",
    "          self.test_sparsity_loss_memory.clear()\n",
    "\n",
    "        if self.wandb_log:\n",
    "          wandb.log({\"test_total_loss\": test_loss})\n",
    "          wandb.log({\"test_reconstruction_loss\": test_rec_loss})\n",
    "          if self.enable_sparsity_loss:\n",
    "            wandb.log({\"test_sparse_loss\": test_sparsity_loss})\n",
    "\n",
    "        self.test_rec_loss = test_rec_loss\n",
    "\n",
    "        print(print_log)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mycondaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
